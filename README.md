
### Work IN Progress Project (Active Project)

Building Neural Networks from scratch using only NUMPY

This project is a work-in-progress implementation of a neural network from scratch. The goal is to build a functional neural network library using only NumPy, for educational purposes.

#### Current Features:
*   **Dense Layer**: A fully connected layer with forward and backward propagation.
*   **Activation Functions**: ReLU, Sigmoid, Softmax, and Linear activations with backward pass.
*   **Loss Functions**: MSE, MAE, SSE, Categorical Cross-Entropy, and Sparse Cross-Entropy.
*   **Backpropagation**: Full implementation of the backward pass.
*   **Optimizers**: An optimizer base class with an initial SGD (Stochastic Gradient Descent) implementation.
*	**High-level Model API**: A Model class to easily build, compile, and train neural networks.


#### Future Goals:
*	**Advanced Optimizers**: Implement more sophisticated optimizers like Adam and RMSprop.
*	**Batch & Data Handling**: Support for mini-batch training and data shuffling for more efficient training on large datasets.
* 	**Metrics & Callbacks**: Include accuracy metrics during training and a callback system for actions like early stopping.
* 	**Advanced Layer Types**: Add more complex layers, such as Convolutional and Pooling layers.
*	**Weight Initialization**: Implement advanced weight initialization techniques like `Xavier` and `He` initialization
* 	**Miscellaneous**: Code refactoring and improvements, better data handling and easier compatibility



